<!doctype html><html lang=zh>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<title>kubernetes - 如创科技</title>
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel=icon href=https://www.rutron.net/favicon.png>
<link rel=stylesheet href=/css/style.min.bed369b6636aa19e87512113b3caaa34b9db89c9c071f25efab970ae54757c9c.css>
</head>
<body class=page>
<div id=main-menu-mobile class=main-menu-mobile>
<ul>
<li class=menu-item-服务>
<a href=/services/>
<span>服务</span>
</a>
</li>
<li class=menu-item-关于>
<a href=/about/>
<span>关于</span>
</a>
</li>
<li class=menu-item-联系>
<a href=/contact/>
<span>联系</span>
</a>
</li>
</ul>
</div>
<div id=wrapper class=wrapper>
<div class="header header-absolute">
<div class=container>
<div class=logo>
<a href=https://www.rutron.net/><img height=40px width=40px alt=如创科技 src=/images/logo.png></a>
</div>
<div class=logo-mobile>
<a href=https://www.rutron.net/><img height=40px width=40px alt=如创科技 src=/images/logo.png></a>
</div>
<div id=main-menu class=main-menu>
<ul>
<li class=menu-item-服务>
<a href=/services/>
<span>服务</span>
</a>
</li>
<li class=menu-item-关于>
<a href=/about/>
<span>关于</span>
</a>
</li>
<li class=menu-item-联系>
<a href=/contact/>
<span>联系</span>
</a>
</li>
</ul>
</div>
<button id=toggle-main-menu-mobile class="hamburger hamburger--slider" type=button aria-label="Toggle Menu">
<span class=hamburger-box>
<span class=hamburger-inner></span>
</span>
</button>
</div>
</div>
<div class="container pt-10 pb-6">
<div class=row>
<div class=col-12>
<h1 class="title-1 black">kubernetes</h1>
</div>
</div>
</div>
<div class="container pb-6">
<div class=row>
<div class="col-12 col-md-6 mb-2"><p>新建集群的第一步就是要规划服务器、网络、操作系统等等, 下面就结合我平时的工作经验总结下相关的要求, 内容根据日常工作持续补充完善:</p>
<h2 id=服务器配置>服务器配置</h2>
<p>kubernetes 集群分为控制节点和数据节点, 它们对于配置的要求有所不同:</p>
<h3 id=控制面>控制面</h3>
<table>
<thead>
<tr>
<th>节点规模</th>
<th>Master规格</th>
</tr>
</thead>
<tbody>
<tr>
<td>1~5个节点</td>
<td>4核 8Gi（不建议2核 4Gi）</td>
</tr>
<tr>
<td>6~20个节点</td>
<td>4核 16Gi</td>
</tr>
<tr>
<td>21~100个节点</td>
<td>8核 32Gi</td>
</tr>
<tr>
<td>100~200个节点</td>
<td>16核 64Gi</td>
</tr>
</tbody>
</table>
<p>系统盘40+Gi，用于储存 etcd 信息及相关配置文件等</p>
<h3 id=数据面>数据面</h3>
<ul>
<li>规格：CPU >= 4核, 内存 >= 8Gi</li>
<li>确定整个集群的日常使用的<strong>总核数</strong>以及<strong>可用度的容忍度</strong>
<ul>
<li>例如：集群总的核数有160核, 可以容忍10%的错误. 那么最小选择10台16核VM, 并且高峰运行的负荷不要超过 <code>160*90%=144核</code>. 如果容忍度是20%, 那么最小选择5台32核VM, 并且高峰运行的负荷不要超过<code>160*80%=128核</code>. 这样就算有一台VM出现故障, 剩余VM仍可以支持现有业务正常运行.</li>
</ul>
</li>
<li>确定 <code>CPU:Memory</code> 比例. 对于使用内存比较多的应用, 例如Java类应用, 建议考虑使用1:8的机型</li>
<li>比如: <code>virtual machine 32C 64G 200G系统盘 数据盘可选</code></li>
</ul>
<h3 id=什么情况下使用裸金属服务器>什么情况下使用裸金属服务器?</h3>
<ul>
<li>集群日常规模能够达到1000核。一台服务器至少96核，这样可以通过10台或11台服务器即可构建一个集群。</li>
<li>快速扩大较多容器。例如：电商类大促，为应对流量尖峰，可以考虑使用裸金属来作为新增节点，这样增加一台裸金属服务器就可以支持很多个容器运行。</li>
</ul>
<h2 id=操作系统>操作系统</h2>
<ul>
<li>建议安装 ubuntu 18.04/debian buster/ubuntu 20.04/centos 7.9 优先级从高到低</li>
<li>linux kenerl 4.17+</li>
<li>安装 ansible v2.9 & python-netaddr 以运行 ansible 命令</li>
<li>安装 jinja 2.11+ 以运行 ansible playbooks</li>
<li>允许 IPv4 forwarding</li>
<li>部署节点的 ssh key 拷贝到所有节点</li>
<li>禁用防火墙</li>
</ul>
<h2 id=网络>网络</h2>
<ul>
<li>单一集群采用统一的网卡命名比如:<code>eth0</code>等, 保证名称唯一</li>
<li>没有特殊要求, 服务器要求可访问外网</li>
</ul>
<h2 id=集群规模限制>集群规模限制</h2>
<ul>
<li>每节点不超过 110 pods</li>
<li>不超过 5k nodes</li>
<li>总计不超过 15w pods</li>
<li>总计不超过 30w containers</li>
</ul>
<h2 id=参考>参考</h2>
<ul>
<li><a href=https://docs.ucloud.cn/uk8s/introduction/node_requirements>优刻得容器云UK8S集群节点配置推荐</a></li>
<li><a href=https://help.aliyun.com/document_detail/98886.html>阿里云ACK容器服务之ECS选型</a></li>
</ul>
</div>
<div class="col-12 col-md-6 mb-2"><p><a href=https://metallb.universe.tf/>官方文档</a></p>
<h2 id=为什么使用>为什么使用?</h2>
<p>Kubernetes没有提供适用于裸金属集群的网络负载均衡器实现, 也就是<code>LoadBalancer</code>类型的Service. Kubernetes 附带的网络负载均衡器的实现都是调用各种 IaaS 平台（GCP、AWS、Azure ……）的胶水代码。 如果您没有在受支持的 IaaS 平台（GCP、AWS、Azure&mldr;）上运行，LoadBalancers 在创建时将一直保持在<code>pending</code>状态。</p>
<p>裸金属集群的运维人员只剩下两个方式来将用户流量引入集群内: <code>NodePort</code>和<code>externalIPs</code>. 这两种在生产环境使用有很大的缺点, 这样, 裸金属集群也就成了 Kubernetes 生态中的第二类选择, 并不是首选.</p>
<p>MetalLB 的目的是实现一个网络负载均衡器来与标准的网络设备集成, 这样这些外部服务就能尽可能的正常工作了.</p>
<h2 id=要求>要求</h2>
<p>MetalLB 要求如下:</p>
<ul>
<li>一个 Kubernetes 集群, Kubernetes 版本 1.13.0+, 没有网络负载均衡器功能.</li>
<li>可以与 MetalLB 共存的集群网络配置。</li>
<li>一些供 MetalLB 分发的 IPv4 地址。</li>
<li>当使用 BGP 操作模式时，您将需要一台或多台能够发布 BGP 的路由器。</li>
<li>使用 L2 操作模式时，节点之间必须允许 7946 端口（TCP 和 UDP，可配置其他端口）上的流量，这是 <a href=https://github.com/hashicorp/memberlist>hashicorp/memberlist</a> 的要求。</li>
</ul>
<h2 id=功能>功能</h2>
<p>MetalLB 是作为 Kubernetes 中的一个组件, 提供了一个网络负载均衡器的实现. 简单来说, 在非公有云环境搭建的集群上, 不能使用公有云的负载均衡器, 它可以让你在集群中创建 LoadBalancer 类型的 Service.</p>
<p>为了提供这样的服务, 它具备两个功能: 地址分配、对外发布</p>
<h3 id=地址分配>地址分配</h3>
<p>在公有云的 Kubernetes 集群中, 你申请一个负载均衡器, 云平台会给你分配一个 IP 地址. 在裸金属集群中, MetalLB 来做地址分配.</p>
<p>MetalLB 不能凭空造 IP, 所以你需要提供供它使用的 IP 地址池. 在 Service 的创建和删除过程中, MetalLB 会对 Service 分配和回收 IP, 这些都是你配置的地址池中的 IP.</p>
<p>如何获取 MetalLB 的 IP 地址池取决于您的环境。 如果您在托管设施中运行裸机集群，您的托管服务提供商可能会提供 IP 地址供出租。 在这种情况下，您将租用例如 /26 的 IP 空间（64 个地址），并将该范围提供给 MetalLB 以用于集群服务。</p>
<p>同样, 如果你的集群是纯私有的, 可以提供一个没有暴露到网络中的相邻的 LAN 网段. 这种情况下, 你可以抽取私有地址空间中的一段 IP 地址, 分配给 MetalLB. 这种地址是免费的, 只要你把它提供给当前 LAN 内的集群服务, 它们就能正常工作.</p>
<p>或者你可以两者都用! MetalLB 可以让你定义多个地址池, 它很方便.</p>
<h3 id=对外发布>对外发布</h3>
<p>当 MetalLB 给 Service 分配一个对外使用的IP之后, 它需要让集群所在的网络知道这个 IP 的存在. MetalLB 使用标准的网络/路由协议来实现, 这要看具体的工作模式: ARP、NDP 或者 BGP.</p>
<h4 id=2层模式-arpndp>2层模式 (ARP/NDP)</h4>
<p>在2层模式下, 集群中的机器使用标准地址发现协议把这些 IPs 通知本地网络. 从 LAN 的角度看, 这台服务器有多个 IP 地址. 这个模式的详细行为还有局限性下面会介绍.</p>
<h4 id=bgp>BGP</h4>
<p>在 BGP 模式下, 集群中的所有机器与临近的路由器建立 BGP 对等会话, 告诉他们如何路由 Service IPs. 得益于 BGP 的策略机制, 使用 BGP 能实现真正的多节点负载均衡和细粒度的流量控制. 下面会介绍更多操作和局限性方面的细节.</p>
<h2 id=工作模式>工作模式</h2>
<h3 id=2层模式>2层模式</h3>
<p>在2层工作模式下, 一个节点负责向本地网络发布服务. 从网络的角度看, 更像是这台服务器的网卡有多个 IP 地址. 在底层, MetalLB 会响应ARP请求(IPv4)和NDP请求(IPv6). 这个工作模式的最大的优势就是适应性强: 它能工作在任何以太网内, 没有特殊的硬件要求, 不需要花哨的路由器.</p>
<h4 id=负载均衡行为>负载均衡行为</h4>
<p>在2层模式下, 发给 Service IP 的流量都会到一个节点上. 在该节点上, <code>kube-proxy</code> 将流量分发到服务具体的 pods 上. L2 并没有实现负载均衡.</p>
<p>但是, 它实现了一个错误转移机制, 当领袖节点因为某些原因故障了, 另一个节点就会接管这些 IP 地址. 故障转移是自动的: 使用 <a href=https://github.com/hashicorp/memberlist>hashicorp/memberlist</a> 检测到节点发生故障, 同时新的节点会从故障节点上接管这些 IP.</p>
<h4 id=局限性>局限性</h4>
<p>L2模式有两个主要局限性: 单点的瓶颈、潜在的缓慢故障转移.</p>
<p>如上面说的, 在 L2 模式下, 选举产生的单一的领袖节点会接收所有的服务流量. 这意味着, 你服务的入口带宽受限与这个单一节点的带宽. 如果使用 ARP/NDP 引导流量, 这是一个基本限制.</p>
<p>当前的实现, 节点之间的故障转移依赖客户端之间的配合. 当故障发生时, MetalLB 会不经请求的发送出一些2层的数据包, 来通知其他客户端 Service IP 所对应的 MAC 地址已经更改.</p>
<p>大多数操作系统能正确处理这种数据包, 同时更新“邻居”的地址缓存. 这种情况下, 故障转移也就几秒钟. 但是, 还是有个别系统要么没有实现这种报文的处理, 要么实现了, 但是更新缓存很慢.</p>
<p>好在所有现代版本的操作系统都正确实现了 L2 故障转移, 比如 Windows、Mac、Linux. 所以, 出问题的仅仅是很老或者不常见的操作系统.</p>
<p>为了最大限度地减少计划内的故障转移对客户端的影响，应该让旧的领袖节点多运行几分钟, 以便它可以继续为旧客户端转发流量，直到它们的缓存刷新。</p>
<p>当一个计划之外的故障出现时, 在访问出错的客户端刷新它们的缓存之前, 这些 Service IPs 将不可达.</p>
<h4 id=和-keepalive-比较>和 keepalive 比较</h4>
<p>MetalLB 的2层模式和 keepalived 有很多相似之处. 所以, 如果你熟悉 keepalived, 我说的很多你应该很熟悉. 但是和它也有一些不同的地方需要说一下.</p>
<p>Keepalived 使用虚拟路由器冗余协议(VRRP). 为了选举领袖并监控该领导者何时离开, keepalived 的各实例之间不断地相互交换 VRRP 消息。</p>
<p>不一样的是, MetalLB 通过 memberlist 来知道什么时候集群中的节点不可达, 什么时候这个节点的 Service IPs 需要移动到别处.</p>
<p>Keepalived 和 MetalLB 从客户端的角度看起来是一样的: 当发生故障转移时，Service IP 地址从一台机器迁移到另一台机器，之后该机器就会有多个 IP 地址。</p>
<p>因为它不用 VRRP, MetalLB 并不会有这个协议本身的局限性. 比如: VRRP协议限制每个网络只能有255个负载均衡器实例, 但是 MetalLB 就没有这个限制. 只要你网络中有空闲IP, 你可以有很多负载均衡器实例.</p>
<p>还有, 配置上 MetalLB 比 Keepalived 少, 比如它不需要 <code>Virtual Router IDs</code>.</p>
<p>另一方面，由于 MetalLB 依赖于 memberlist 来获取集群成员信息，它无法与第三方 VRRP 感知路由器和基础设施进行互操作。 这是MetalLB的定位: MetalLB 专门设计<strong>用于在 Kubernetes 集群内</strong>提供负载平衡和故障转移.</p>
<h3 id=bgp模式>BGP模式</h3>
<p>在 BGP 模式下，集群中的每个节点都会与您的网络路由器建立 BGP 对等会话，并使用该对等会话来通告外部集群服务的 IP.</p>
<p>假设您的路由器配置为支持多路径，这将实现真正的负载平衡: MetalLB 发布的路由彼此等效。这意味着路由器将使用所有下一跳，并在它们之间进行负载平衡.</p>
<p>数据包到达节点后，kube-proxy 负责流量路由的最后一跳，将数据包送到服务中的特定 pod.</p>
<h4 id=负载均衡行为-1>负载均衡行为</h4>
<p>负载均衡的确切行为取决于您的特定路由器型号和配置，但常见的行为是基于 <code>packet-hash</code> 方法在连接层面 <code>per-connection</code> 进行平衡。这是什么意思?</p>
<p>这个<code>per-connection</code>意味着单个 TCP 或 UDP 会话的<strong>所有数据包</strong>将被定向到集群中的单个机器。流量传播只发生在不同的连接之间，而不是一个连接内的数据包. 这是一件好事，因为在多个集群节点上传播数据包会导致一些问题：</p>
<ul>
<li>跨多个传输路径传播单个连接会导致数据包(packet)在线路上重新排序，这会极大地影响终端主机的性能。</li>
<li>在 Kubernetes 中, 不能保证节点之间流量的路由保持一致。这意味着两个不同的节点可以将同一连接的数据包(packet)路由到不同的 Pod，这将导致连接失败。</li>
</ul>
<p>高性能路由器能够以一种无状态的方式在多个后端之间使用数据包哈希的方法分发数据包. 对于每一个数据包, 它们拥有一些属性, 并能用它作为 “种子” 来决定选择哪一个后端. 如果, 所有的属性都一样, 它们就会选择同一个后端.</p>
<p>具体使用哪种哈希方法取决于路由器的硬件和软件. 典型的方法是: <code>3-tuple</code> 和 <code>5-tuple</code>. 3-tuple 哈希法使用数据包中的协议、源IP、目的IP作为哈希键, 这意味着来自不同ip的数据包会进入同一个后端. 5-tuple 哈希法又在其中加入了源端口和目的端口, 所有来自相同客户端的不同连接将会在集群中均衡分布.</p>
<p>通常, 我们推荐加入尽可能多的属性来参与数据包哈希, 也就是说使用更多的属性是非常好的. 因为这样会更加接近理想的负载均衡状态, 每一个节点都会收到相同数量的数据包. 但是我们永远不会达到这种理想状态, 因为上述原因, 但是我们能做的就是尽可能的均匀的传播连接, 以防止出现主机热点.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#75715e># 一个连接(connection)由多个连续的数据包(packet)构成</span>
<span style=color:#75715e># 比如: </span>
connection <span style=color:#ae81ff>1</span> : source<span style=color:#f92672>[</span>ip:port<span style=color:#f92672>]</span> -packet N-&gt;...-packet 1-&gt; target<span style=color:#f92672>[</span>ip:port<span style=color:#f92672>]</span>
connection <span style=color:#ae81ff>2</span> : source<span style=color:#f92672>[</span>ip:port<span style=color:#f92672>]</span> -packet N-&gt;...-packet 1-&gt; target<span style=color:#f92672>[</span>ip:port<span style=color:#f92672>]</span>
</code></pre></div><h4 id=局限性-1>局限性</h4>
<p>使用 BGP 作为负载均衡机制可以让你使用标准的路由器硬件, 而不是定制的负载均衡器. 但是, 这也带来的一些缺点:</p>
<p>最大的缺点是基于 BGP 的负载平衡不能优雅地响应后端设置的地址更改. 也就是说, 当集群的一个节点下线了, 到你服务的所有成功的连接都会损坏(用户会看到报错:<code>Connection reset by peer</code>)</p>
<p>基于 BGP 的路由器实现无状态负载均衡。 他们通过哈希数据包头中的一些字段并将该哈希用作可用后端数组的索引，将给定数据包分配给特定的下一跳。</p>
<p>问题是路由器中使用的哈希值通常<em>不稳定</em>，所以每当后端集的大小发生变化时（例如，当一个节点的 BGP 会话关闭时），现有的连接将被有效地随机重新哈希，这意味着大多数现有的 连接最终会突然被转发到不同的后端，一个不知道相关连接的后端。</p>
<p>结果是每当你的服务的 <code>IP > Node</code> 映射发生变化时, 你希望看到一次性干净的切换出现, 到该服务的大部分所有可用连接中断. 没有持续的丢包或者黑洞, 只是一次很干净的中断而已.</p>
<p>根据您的服务的用途，您可以采用几种缓解策略：</p>
<ul>
<li>你的 BGP 路由器可能有更加稳定的ECMP哈希算法. 有时候可能叫: <code>弹性ECMP</code> 或 <code>弹性LAG</code>. 使用这样的算法, 在后端集合发生变化的时候, 能有效的减少受影响的连接数量.</li>
<li>把这些需要外部访问的服务部署到特定的节点上, 来减小节点池的大小, 平时看好这些节点.</li>
<li>把服务的变更安排在流量的低谷时候, 此时你的用户在睡觉流量很低.</li>
<li>把每一个逻辑上的服务拆分成两个有着不同 IP 的 kubernetes 服务, 使用DNS服务优雅的将用户流量从将要中断的服务迁移到另一个服务上.</li>
<li>在客户端添加重试逻辑, 以优雅地从突然断开连接中恢复. 如果您的客户是移动应用程序或丰富的单页网络应用程序, 这尤其适用.</li>
<li>将您的服务放在 ingress 控制器后面. ingress 控制器本身可以使用 MetalLB 来接收流量, 但是在 BGP 和您的服务之间有一个状态层意味着您可以毫无顾虑地更改您的服务。 您只需在更改 ingress 控制器本身的部署时小心(例如, 在添加更多 NGINX pod 时).</li>
<li>接受偶尔会出现重置连接的情况. 对于低可用性的内部服务, 这可能是可以接受的.</li>
</ul>
<h4 id=frr-模式>FRR 模式</h4>
<p>MetalLB 提供了一个实验模式: 使用 FRR 作为 BGP 层的后端.</p>
<p>开启 FRR 模式之后, 会获得以下额外的特性:</p>
<ul>
<li>由BFD支持的BGP会话</li>
<li>BGP和BFD都支持IPv6</li>
<li>多协议支持的BGP</li>
</ul>
<h5 id=frr-模式的局限性>FRR 模式的局限性</h5>
<p>相比与原生实现, FRR 模式有以下局限性:</p>
<ul>
<li><code>BGPAdvertisement</code> 的 <code>RouterID</code> 字段可以被覆盖，但它必须对所有的 advertisements 都相同（不能有不同的 advertisements 具有不同的 RouterID）。</li>
<li><code>BGPAdvertisement</code> 的 <code>myAsn</code> 字段可以被覆盖，但它必须对所有 advertisements 都相同(不能有不同的 advertisements 具有不同的 myAsn)</li>
<li>如果 eBGP Peer 是距离节点多跳的, 则 <code>ebgp-multihop</code> 标志必须设置为 <code>true</code></li>
</ul>
<h2 id=安装>安装</h2>
<p>安装之前, 确保满足所有<a href=#%E8%A6%81%E6%B1%82>要求</a>. 尤其是, 你要注意<a href=#%E7%BD%91%E7%BB%9C%E9%99%84%E5%8A%A0%E7%BB%84%E4%BB%B6%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7>网络附加组件的兼容性</a></p>
<p>如果你在云平台环境运行 MetalLB, 你最好先看看<a href=https://metallb.universe.tf/installation/clouds/>云环境兼容性</a>页面, 确保你选择的云平台可以和 MetalLB 一起正常工作(大多数情况下都不好用).</p>
<p>MetalLB 支持4种安装方式:</p>
<ul>
<li>使用 Kubernetes 部署清单安装</li>
<li>使用 Kustomize 安装</li>
<li>使用 Helm 安装</li>
<li>使用 MetalLB Operator 安装</li>
</ul>
<h3 id=准备>准备</h3>
<p>如果您在 IPVS 模式下使用 kube-proxy，则从 Kubernetes v1.14.2 开始，您必须启用严格的 ARP 模式。</p>
<blockquote>
<p>请注意，如果您使用 kube-router 作为服务代理，则不需要这个，因为它默认启用严格的 ARP。</p>
</blockquote>
<p>你可以在集群中修改 kube-proxy 的配置文件:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit configmap -n kube-system kube-proxy
</code></pre></div><p>这样配置:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeproxy.config.k8s.io/v1alpha1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>KubeProxyConfiguration</span>
<span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;ipvs&#34;</span>
<span style=color:#f92672>ipvs</span>:
  <span style=color:#f92672>strictARP</span>: <span style=color:#66d9ef>true</span>
</code></pre></div><p>你也可以把这个配置片段加入到你的 <code>kubeadm-config</code> 中, 在主配置后面使用 <code>---</code> 附加上它就行.</p>
<p>如果你想自动更改配置, 下面的 shell 脚本可以用:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#75715e># 查看将会发生什么样的配置变化, 如果存在不同则返回非零返回值</span>
kubectl get configmap kube-proxy -n kube-system -o yaml | <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>sed -e <span style=color:#e6db74>&#34;s/strictARP: false/strictARP: true/&#34;</span> | <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>kubectl diff -f - -n kube-system

<span style=color:#75715e># 实际应用变更, 仅在遇到错误的时候返回非零返回值</span>
kubectl get configmap kube-proxy -n kube-system -o yaml | <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>sed -e <span style=color:#e6db74>&#34;s/strictARP: false/strictARP: true/&#34;</span> | <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>kubectl apply -f - -n kube-system
</code></pre></div><h3 id=使用部署清单安装>使用部署清单安装</h3>
<p>使用下面的部署清单, 来安装MetalLB:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/
metallb/metallb/v0.13.4/config/manifests/metallb-native.yaml
</code></pre></div><blockquote>
<p>如果你想开启使用实验性的FRR模式, 使用下面的部署清单
<code>kubectl apply -f https://raw.githubusercontent.com</code>
<code>/metallb/metallb/v0.13.4/config/manifests/metallb-frr.yaml</code>
注意: 这些部署清单来自开发分支. 如果应用在生产环境, 强烈推荐你使用稳定的发布版本.</p>
</blockquote>
<p>这样, 在 <code>metallb-system</code> 名称空间下就部署了 MetalLB. 主要组件是:</p>
<ul>
<li>Deployment: <code>metallb-system/controller</code>, 这是集群级别的控制器, 负责处理 IP 分配.</li>
<li>Daemonset: <code>metallb-system/speaker</code>, 这个组件使用你选择的协议对外发送信息, 使你的 Service 可以被访问.</li>
<li>ServiceAccount: controller 和 speaker 所使用的, 同时配置好了组件所需要的 RBAC 权限.</li>
</ul>
<p>该安装清单并不包含配置文件, 但是 MetalLB 组件仍会启动, 在你<a href=#%E9%85%8D%E7%BD%AE>部署相关配置资源</a>之前, 它保持空闲状态.</p>
<h3 id=其他安装方式>其他安装方式</h3>
<p>另外的三种安装方式(Kustomize, Helm, MetalLB Operator)请查看<a href=https://metallb.universe.tf/installation/>官方文档</a></p>
<h3 id=网络附加组件的兼容性>网络附加组件的兼容性</h3>
<p>通常来说, MetalLB 并不关心你集群用什么网络附件组件, 只要它能满足 Kubernetes 要求的标准就可以.</p>
<p>下面是一些网络附加组件与 MetalLB 一起测试的结果, 可以供你参考. 列表中没有的附加组件可能也可以正常工作, 只是我们没有测试.</p>
<table>
<thead>
<tr>
<th>网络附加组件</th>
<th>兼容性</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Antrea</td>
<td>可以</td>
<td>1.4和1.5版本测试通过</td>
</tr>
<tr>
<td>Calico</td>
<td>大部分可以</td>
<td>Calico也提供了使用BGP公布LoadBalancer IP的能力 <a href=https://metallb.universe.tf/configuration/calico/>详细</a></td>
</tr>
<tr>
<td>Canal</td>
<td>可以</td>
<td></td>
</tr>
<tr>
<td>Cilium</td>
<td>可以</td>
<td></td>
</tr>
<tr>
<td>Flannel</td>
<td>可以</td>
<td></td>
</tr>
<tr>
<td>Kube-ovn</td>
<td>可以</td>
<td></td>
</tr>
<tr>
<td>Kube-router</td>
<td>大部分可以</td>
<td><a href=https://metallb.universe.tf/configuration/kube-router/>已知问题</a></td>
</tr>
<tr>
<td>Weave Net</td>
<td>大部分可以</td>
<td><a href=https://metallb.universe.tf/configuration/weave/>已知问题</a></td>
</tr>
</tbody>
</table>
<h2 id=配置>配置</h2>
<p>在配置之前, MetalLB一直保持空闲状态. 想要配置 MetalLB, 需要在 MetalLB 所在的名称空间(通常是 <code>metallb-system</code>)部署很多和配置相关的资源.</p>
<p>当然, 如果你没有把 MetalLB 部署到 <code>metallb-system</code> 名称空间下, 你可能需要修改下面的配置清单.</p>
<h3 id=为loadbalancer类型服务定义可分配的ip地址>为<code>LoadBalancer</code>类型服务定义可分配的IP地址</h3>
<p>为了能给 Services 分配 IPs, MetalLB 通过 <code>IPAddressPool</code> 自定义资源来定义.</p>
<p>通过 <code>IPAddressPools</code> 定义好 IPs 地址池之后, MetalLB 就会使用这些地址给 Services 分配 IP.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPAddressPool</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>first-pool</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>addresses</span>:
  - <span style=color:#ae81ff>192.168.10.0</span><span style=color:#ae81ff>/24</span>
  - <span style=color:#ae81ff>192.168.9.1-192.168.9.5</span>
  - <span style=color:#ae81ff>fc00:f853:0ccd:e799::/124</span>
</code></pre></div><p>可以同时定义多个 <code>IPAddressPools</code> 资源. 可以使用 CIDR 定义地址, 也可以使用范围区间定义, 也可以定义 IPv4 和 IPv6 地址用于分配.</p>
<h3 id=对外公布service-ips>对外公布Service IPs</h3>
<p>一旦给 Service 分配IPs之后, 它们必须要公布出去. 具体的配置要根据你想使用的协议来定, 下面会一一介绍:</p>
<p><em>注意: 也可以同时将一个 service 同时通过 L2 和 BGP 对外公布.</em></p>
<h3 id=使用-l2-模式的配置>使用 L2 模式的配置</h3>
<p>配置2层模式最简单, 在多数情况下, 你不需要任何关于协议的配置, 只配置IP地址就行.</p>
<p>使用2层模式<strong>不需要将IP地址绑定到工作节点的网络接口上</strong>. 它直接响应本地网络上的ARP请求，将机器的MAC地址提供给客户端。</p>
<p>为了公布来自 <code>IPAddressPool</code> 的 IP, <code>L2Advertisement</code> 资源必须与 <code>IPAddressPool</code> 资源一起使用.</p>
<p>比如, 下面的例子配置了 MetalLB 可以分配从 192.168.1.240 到 192.168.1.250 之间的地址, 并配置它使用2层模式:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPAddressPool</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>first-pool</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>addresses</span>:
  - <span style=color:#ae81ff>192.168.1.240-192.168.1.250</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>L2Advertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
</code></pre></div><p>上面的例子, 在 <code>L2Advertisement</code> 中没有配置 <code>IPAddressPool</code> 选择器, 这样它能使用所有的 IP 地址.</p>
<p>所以, 为了只将一部分 <code>IPAddressPool</code>s 使用 L2 对外公布, 我们就需要声明一下(或者, 使用标签选择器).</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>L2Advertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>ipAddressPools</span>:
  - <span style=color:#ae81ff>first-pool</span>
</code></pre></div><h3 id=使用-bgp-模式的配置>使用 BGP 模式的配置</h3>
<p>需要告诉 MetalLB 如何与外部一个或多个 BGP 路由器建立会话.</p>
<p>所以, 需要给每一个 MetalLB 需要连接到路由器建一个 <code>BGPPeer</code> 资源.</p>
<p>为了能提供一个基础的 BGP 路由器配置和一个 IP 地址范围, 你需要定义4段信息.</p>
<ul>
<li>MetalLB 需要连接的路由器地址</li>
<li>路由器的AS编号</li>
<li>MetalLB 使用的AS编号</li>
<li>使用CIDR前缀表示IP地址范围</li>
</ul>
<p>比如给MetalLB分配的AS编号为64500, 并且将它连接到地址为 <code>10.0.0.1</code> 且AS编号为 64501 的路由器，您的配置将如下所示：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta2</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPPeer</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sample</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>myASN</span>: <span style=color:#ae81ff>64500</span>
  <span style=color:#f92672>peerASN</span>: <span style=color:#ae81ff>64501</span>
  <span style=color:#f92672>peerAddress</span>: <span style=color:#ae81ff>10.0.0.1</span>
</code></pre></div><p>提供一个IP地址池 <code>IPAddressPool</code> 像这样:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPAddressPool</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>first-pool</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>addresses</span>:
  - <span style=color:#ae81ff>192.168.1.240-192.168.1.250</span>
</code></pre></div><p>使用自定义资源<code>BGPAdvertisement</code>来配置MetalLB使用BGP来公布IPs:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPAdvertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
</code></pre></div><p>在<code>BGPAdvertisement</code>的定义中, 如果没有配置<code>IPAddressPool</code>选择器, 默认使用所有可用的IP地址池<code>IPAddressPools</code></p>
<p>如果仅需要使用特定的IP地址池通过BGP进行地址公布, 则需要声明一个IP地址池列表<code>ipAddressPools</code>(或者使用标签选择器):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPAdvertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>ipAddressPools</span>:
  - <span style=color:#ae81ff>first-pool</span>
</code></pre></div><h3 id=为-bgp-会话开启-bfd-支持>为 BGP 会话开启 BFD 支持</h3>
<p>在实验的FRR模式下, BGP会话可以由BFD会话取代，从而能实现比单纯使用BGP更快的路径故障检测的能力.</p>
<p>想要开启BFD, 你必须定义个BFD配置<code>BFDProfile</code>, 并且和BGP会话对等方配置<code>BGPPeer</code>关联起来:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BFDProfile</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>testbfdprofile</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>receiveInterval</span>: <span style=color:#ae81ff>380</span>
  <span style=color:#f92672>transmitInterval</span>: <span style=color:#ae81ff>270</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta2</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPPeer</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>peersample</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>myASN</span>: <span style=color:#ae81ff>64512</span>
  <span style=color:#f92672>peerASN</span>: <span style=color:#ae81ff>64512</span>
  <span style=color:#f92672>peerAddress</span>: <span style=color:#ae81ff>172.30.0.3</span>
  <span style=color:#f92672>bfdProfile</span>: <span style=color:#ae81ff>testbfdprofile</span>
</code></pre></div><h3 id=配置验证>配置验证</h3>
<p>部署时 MetalLB 会安装配置验证的钩子程序 <code>validatingwebhook</code>, 用来检查用户部署的自定义资源的有效性.</p>
<p>但是, 因为 MetalLB 的整体配置被分隔成很多分片, 不是所有的无效配置都能被钩子程序避免. 所以, 一旦无效的 MetalLB 配置资源被成功部署了, MetalLB 会忽略它, 并使用最新的有效配置.</p>
<p>未来的版本中, MetalLB 会把错误的配置暴露到 Kubernetes 资源中. 但是, 目前来说, 如果需要知道为什么配置没有生效, 就需要去查看一下控制器的日志.</p>
<h3 id=更多高级配置>更多高级配置</h3>
<p>关于地址分配、BGP、L2、calico等相关的高级配置, 请参考<a href=https://metallb.universe.tf/configuration/_advanced_ipaddresspool_config/>官方文档</a></p>
<h2 id=如何使用>如何使用</h2>
<p>当安装并配置完 MetalLB 之后, 为了对外暴露 Service, 非常简单, 将 Service 的 <code>spec.type</code> 配置为 <code>LoadBalancer</code>, 剩下的就交给 MetalLB 就好了.</p>
<p>MetalLB 会给它控制的 Service 添加一些事件, 如果你的 <code>LoadBalancer</code> 类型的 service 表现的不符合预期, 可以执行<code>kubectl describe service &lt;service name></code>查看事件日志.</p>
<h3 id=请求特定的ips>请求特定的IPs</h3>
<p>MetalLB 尊重<code>spec.loadBalancerIP</code>参数, 所以, 如果你想要使用一个特定的IP地址, 你可以通过配置这个参数来指定. 如果MetalLB没有你申请的IP地址, 或者如果你申请的IP地址已经被其他的服务占用了, 地址分配就会失败, MetalLB会记录一个Warning级别的事件, 你通过<code>kubectl describe service &lt;service name></code>就能看到.</p>
<p>MetalLB不仅支持<code>spec.loadBalancerIP</code>参数, 还支持一个自定义 annotation 参数: <code>metallb.universe.tf/loadBalancerIPs</code>. 对于有些双栈的 Service 需要多个IPs, 这个 annotation 也支持使用逗号分隔指定多个IPs</p>
<p><strong>请注意</strong>: 在 kubernetes 的API中, <code>spec.LoadBalancerIP</code>参数未来计划会被废弃. <a href=https://github.com/kubernetes/kubernetes/pull/107235>请看这</a></p>
<p>如果你想使用特定的类型的IP, 但是不在乎具体是什么地址, MetalLB同样也支持请求一个特定的IP地址池. 为能能使用特定的地址池, 你需要在 Service 中添加一个 annotation: <code>metallb.universe.tf/address-pool</code>, 来指定IP地址池的名称. 比如:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
  <span style=color:#f92672>annotations</span>:
    <span style=color:#f92672>metallb.universe.tf/address-pool</span>: <span style=color:#ae81ff>production-public-ips</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>ports</span>:
  - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>80</span>
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
</code></pre></div><h3 id=流量策略>流量策略</h3>
<p>MetalLB 理解并尊重服务的 <code>externalTrafficPolicy</code> 选项，并根据您选择的策略和公告协议实现不同的公告模式。</p>
<h4 id=layer2>Layer2</h4>
<p>当使用2层模式公布的时候, 集群中的一个节点会接收给 Service IP 的流量. 从那开始, 行为就取决于选择的流量策略.</p>
<h5 id=cluster流量策略><code>Cluster</code>流量策略</h5>
<p>使用默认的<code>Cluster</code>流量策略, 节点上的<code>kube-proxy</code>接收流量并负载均衡, 并且将流量分发给 Service 对应的 Pod.</p>
<p>这种策略下 pods 之间的流量是均匀分布的. 但是<code>kube-proxy</code>在进行负载均衡的时候会隐藏真实的源IP地址, 因此, 在 pod 的日志中会看到外部流量来自 MetalLB 的领袖节点.</p>
<h5 id=local流量策略><code>Local</code>流量策略</h5>
<p>使用<code>Local</code>流量策略, 节点上的<code>kube-proxy</code>接收流量, 同时将流量发送给当前节点的 pod. 因为 kube-proxy 不会跨集群节点分发流量, 你的 pod 可以看到真实的源IP地址.</p>
<p>这个策略的缺点是, 流量仅能流向 Service 对应的某些 pod. 那些不在领袖节点上的 Pod 是无法接收到任何流量的, 他们可以暂时当作副本存在, 当 MetalLB 发生故障转移的时候他们或许可以接收流量.</p>
<h4 id=bgp-1>BGP</h4>
<p>当通过 BGP 发布时, MetalLB尊重Service的<code>externalTrafficPolicy</code>选项, 按照用户选择的策略实现了两种不同的发布模式.</p>
<h5 id=cluster流量策略-1><code>Cluster</code>流量策略</h5>
<p>使用默认的<code>Cluster</code>流量策略, 集群中的每个节点都会接收服务 IP 的流量。 在每个节点上，流量都经过第二次负载均衡（由 kube-proxy 提供），它将流量引导到各个 pod。</p>
<p>此策略会在集群中的所有节点以及服务中的所有 Pod 之间实现统一的流量分布。 但是，因为存在两次负载均衡（一次在 BGP 路由器上，一次在节点上的 <code>kube-proxy</code> 上），这会导致流量低效。 例如，特定用户的连接可能由 BGP 路由器发送到节点 A，但随后节点 A 决定将该连接发送到运行在节点 B 上的 pod。</p>
<p><code>Cluster</code>策略的另一个缺点是 <code>kube-proxy</code> 在进行负载平衡时会隐藏连接的源 IP 地址，因此在 pod 日志中会看到外部流量来自集群的节点。</p>
<h5 id=local流量策略-1><code>Local</code>流量策略</h5>
<p>使用<code>Local</code>流量策略，只有在本地运行了一个或多个 Services 的 Pod 时, 该节点才会吸引流量。 同时 BGP 路由器仅在托管服务的那些节点之间, 对传入流量进行负载平衡。在每个节点上，流量仅通过 <code>kube-proxy</code> 转发到本地 Pod，节点之间没有“水平”流量。</p>
<p>此策略为你的服务提供最有效的流量。此外，由于 <code>kube-proxy</code> 不需要在集群节点之间发送流量，因此您的 pod 可以看到传入连接的真实源 IP 地址。</p>
<p>该策略的缺点是: 节点作为负载均衡的一个单元，它不管该节点上运行了多少服务的 pod。所以, 这可能会导致你的 pod 流量不平衡。</p>
<p>比如，如果你有一个服务, 它在节点 A 上运行 2 个 pod，在节点 B 上运行1个 pod，则<code>Local</code>流量策略会将到该服务的流量平分到这两个节点(A&B节点各50%)。在节点A上, 又会将到达A的流量平分给2个 pod, 因此节点A上的每个 pod 的负载分配为 25%，节点B的 pod 为 50%。相反，如果您使用<code>Cluster</code>流量策略，每个 pod 将接收到总流量的 33%。</p>
<p>一般来说，在使用 <code>Local</code> 流量策略时，建议对 Pod 在节点上的调度进行精细控制，例如使用节点反亲和性，从而实现 Pod 之间的流量均匀.</p>
<p>将来，MetalLB 或许会解决这种流量策略的缺点，那时, 它无疑会成为 BGP 模式一起使用的最佳模式。</p>
<h3 id=ipv6和双协议栈services>IPv6和双协议栈Services</h3>
<p>在 L2 模式下同时支持 IPv6 和双协议栈Services，但在 BGP 模式下仅通过实验性 FRR 模式来提供支持.</p>
<p>为了让 MetalLB 将 IP 分配给双栈服务，必须至少有一个IP地址池同时具有 v4 和 v6 版本的地址。</p>
<p>请注意，在双协议栈Services的情况下，不能使用<code>spec.loadBalancerIP</code>，因为它不允许请求多个IP，因此必须使用注解 <code>metallb.universe.tf/loadBalancerIPs</code>。</p>
<h3 id=ip地址共享>IP地址共享</h3>
<p>默认, Services 之间不能共享IP地址. 如果你希望多个Service使用一个IP地址. 你可以在 service 上配置 annotation <code>metallb.universe.tf/allow-shared-ip</code> 来开启优选择的IP地址共享.</p>
<p>这个 annotation 的值是一个共享 key. 下面几种情况下, Services 可以共享IP:</p>
<ul>
<li>他们都有一个相同的共享 key.</li>
<li>他们使用的端口不一样(比如一个是 tcp/80 另一个是 tcp/443)</li>
<li>他们都使用<code>Cluster</code>外部流量策略, 或者它们都指向相同的一组 pods(比如 pod 选择器是完全相同的)</li>
</ul>
<p>如果这些条件不满足, MetalLB可能会给两个 service 分配同一个IP, 但是也不一定. 如果你想确保多个 service 共享一个特定的IP, 使用上面提到的<code>spec.loadBalancerIP</code>来定义.</p>
<p>以这样的方式来管理 Service 有两个主要原因: 1. 规避 Kubernetes 的限制; 2. 可使用的IP地址有限.</p>
<p>下面是两个 services 共享一个IP地址的例子:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dns-service-tcp</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
  <span style=color:#f92672>annotations</span>:
    <span style=color:#f92672>metallb.universe.tf/allow-shared-ip</span>: <span style=color:#e6db74>&#34;key-to-share-1.2.3.4&#34;</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
  <span style=color:#f92672>loadBalancerIP</span>: <span style=color:#ae81ff>1.2.3.4</span>
  <span style=color:#f92672>ports</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dnstcp</span>
      <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>53</span>
      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>53</span>
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>dns</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dns-service-udp</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
  <span style=color:#f92672>annotations</span>:
    <span style=color:#f92672>metallb.universe.tf/allow-shared-ip</span>: <span style=color:#e6db74>&#34;key-to-share-1.2.3.4&#34;</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
  <span style=color:#f92672>loadBalancerIP</span>: <span style=color:#ae81ff>1.2.3.4</span>
  <span style=color:#f92672>ports</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dnsudp</span>
      <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>UDP</span>
      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>53</span>
      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>53</span>
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>dns</span>
</code></pre></div><p><a href=https://github.com/kubernetes/kubernetes/issues/23880>目前 kubernetes 不支持多协议的 LoadBalancer Service</a>. 通常, 像DNS这样的服务, 会同时监听TCP和UDP. 为了规避这个限制. 创建两个 Service(一个使用TCP, 一个使用UDP), 它们使用同样的pod选择器. 然后给他们配置相同的共享Key和<code>spec.loadBalancerIP</code>, 这样就可以在同一个IP地址上同时使用TCP和UDP.</p>
<p>第二个原因很简单, 如果你的Service数量比IP地址数量多, 并且也搞不来更多的IP地址. 那么只能共享IP地址了.</p>
<h2 id=一些例子>一些例子</h2>
<p>假如, 一个电子商务平台由一个生产环境和很多沙箱环境. 生产环境需要公网IP地址, 但是沙箱环境使用私有的IP地址, 开发者通过VPN可以访问沙箱环境.</p>
<p>另外, 生产的IP已经在很多地方写死了(比如, DNS、安全扫描等), 所以在生产环境中, 我们希望特定的服务使用特定的IP地址. 因为沙箱环境是由开发者开启和关闭的, 所以我们不想手动管理.</p>
<p>我们可以使用 MetalLB 来满足上面的要求, 我们定义两个IP地址池, 通过定义BGP属性来控制每一个IP地址池的可见性.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPAddressPool</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>production</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#75715e># 生产使用,因为公网的IP很贵,我们只有4个</span>
  <span style=color:#f92672>addresses</span>:
  - <span style=color:#ae81ff>42.176.25.64</span><span style=color:#ae81ff>/30</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPAddressPool</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sandbox</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>addresses</span>:
  <span style=color:#75715e># 相反, 沙箱环境使用私有IP空间</span>
  <span style=color:#75715e># 免费的而且很多, 所以我们给这个地址池分配了大量的IP</span>
  <span style=color:#75715e># 这样开发者可以根据需要启动很多沙箱环境</span>
  - <span style=color:#ae81ff>192.168.144.0</span><span style=color:#ae81ff>/20</span>
</code></pre></div><p>然后我们需要公布他们, 可以通过设置BGP的一些属性来控制每一个地址集合的可见性.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPAdvertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>external</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>ipAddressPools</span>:
  - <span style=color:#ae81ff>production</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BGPAdvertisement</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>local</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>ipAddressPools</span>:
  - <span style=color:#ae81ff>sandbox</span>
  <span style=color:#f92672>communities</span>:
    - <span style=color:#ae81ff>vpn-only</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#75715e># 我们数据中心路由器知道“vpn-only”的BGP社区。</span>
<span style=color:#75715e># 带有此社区标签的公告将仅通过公司VPN隧道传播回开发人员办公室。</span>
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>metallb.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Community</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>communities</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>metallb-system</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>communities</span>:
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>vpn-only</span>
    <span style=color:#f92672>value</span>: <span style=color:#ae81ff>1234</span>:<span style=color:#ae81ff>1</span>
</code></pre></div><p>在我们沙箱环境的 Helm 定义 charts 中, 我们给每一个 service 都打上了annotation <code>metallb.universe.tf/address-pool: sandbox</code>. 这样, 不管开发者什么时候创建沙箱环境, 都会从<code>192.168.144.0/20</code>获得一个IP地址.</p>
<p>对于生产环境, 我们使用<code>spec.loadBalancerIP</code>参数来给 service 定义特定的IP地址.</p>
<hr>
<pre tabindex=0><code>Copyright © The MetalLB Contributors.
Copyright © 2021 The Linux Foundation ®. 
All rights reserved. Linux 基金会已注册商标并使用商标.
</code></pre></div>
<div class="col-12 col-md-6 mb-2"><p>每个人都听过容器，但它究竟是什么？</p>
<p>支持这项技术的软件有很多，其中 Docker 最为流行。因为它的可移植性和环境隔离的能力，它在数据中心内部特别流行。为了能理解这个技术，需要理解很多方面。</p>
<p>注意：很多人拿容器和虚拟机比较，他们有不同的设计目标，不是替代关系，重叠度很小。容器旨在成为一个轻量级环境，您可以裸机上启动容器，托管一个或几个独立的应用程序。当您想要托管整个操作系统或生态系统或者可能运行与底层环境不兼容的应用程序时，您应该选择虚拟机。</p>
<h2 id=linux-控制组>Linux 控制组</h2>
<p>说实话，零信任环境下有些软件的确需要被控制或被限制 - 至少为了稳定，或是为了安全。很多时候一个Bug或不良代码可能会摧毁整个机器并削弱整个生态系统。还好，有办法来控制这些应用程序，控制组(cgroups)是内核的一个特性，它能限制/计量/隔离一个或者多个进程使用CPU、内存、磁盘I/O和网络。<br>
cgroup技术最开始是Google开发，最终在2.6.24版本（2008年1月）的内核中出现。3.15和3.16版本内核将合并进重新设计的cgroups，它添加了kernfs(拆分一些sysfs逻辑)。<br>
cgroups的主要设计目标是提供一个统一的接口，它可以管理进程或者整个操作系统级别的虚拟化，包含Linux容器，或者LXC。cgroups主要提供了以下能力：</p>
<ul>
<li><strong>资源限制</strong>：一个组，可以通过配置使其不能使用超过特定内存限制，或者使用超过指定数量的处理器，或者被限制使用特定的外围设备。</li>
<li><strong>优先级</strong>：可以配置一个或多个组比别的组使用更少/更多的CPU或者I/O吞吐。</li>
<li><strong>计量</strong>：组的资源使用是被监控和计量的。</li>
<li><strong>控制</strong>：进程组可以被冻结、停止或重启。</li>
</ul>
<p>一个 cgroup 可以由一个或多个进程组成，这些进程都绑定到同一组限制。这些组也可以是分层的，这意味着子组继承了对其父组管理的限制。<br>
Linux内核为cgroups提供了一系列控制器或者子系统，控制器负责给一个或者一组进程分配指定的系统资源。比如，<code>memory</code>控制器限制内存使用，<code>cpuacct</code>控制器限制cpu使用。<br>
您可以直接或间接访问和管理 cgroup（使用 LXC、libvirt 或 Docker），首先，我在这里通过 sysfs 和 <code>libcgroups</code> 库介绍。下面的例子中，需要安装必要的软件包。在Red Hat Enterprise Linux或者CentOS上，执行下面命令：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo yum install libcgroup libcgroup-tools
</code></pre></div><p>在Ubuntu或Debian上这样安装：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get install libcgroup1 cgroup-tools
</code></pre></div><p>这个例子中，我用一个简单的脚本(test.sh)，里面会执行一个无限循环。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ cat test.sh
<span style=color:#75715e>#!/bin/sh</span>

<span style=color:#66d9ef>while</span> <span style=color:#f92672>[</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>]</span>; <span style=color:#66d9ef>do</span>
    echo <span style=color:#e6db74>&#34;hello world&#34;</span>
    sleep <span style=color:#ae81ff>60</span>
<span style=color:#66d9ef>done</span>
</code></pre></div><h2 id=手动方式>手动方式</h2>
<p>需要的软件包安装完毕之后，您可以通过 <strong>sysfs 层次结构</strong>直接配置您的 cgroup。比如，要在<code>memory</code>子系统下创建一个名为 <code>foo</code> 的 cgroup，请在 <code>/sys/fs/cgroup/memory</code> 中创建一个名为 foo 的目录：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir /sys/fs/cgroup/memory/foo
</code></pre></div><p>默认情况下，每个新创建的 cgroup 都将继承对系统整个内存池的访问权限。但是，对于那些不断分配内存却不释放的应用来说，这样并不好。要将应用程序限制在合理的范围内，您需要更新 memory.limit_in_bytes 文件。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ echo <span style=color:#ae81ff>50000000</span> | sudo tee
 ↪/sys/fs/cgroup/memory/foo/memory.limit_in_bytes
</code></pre></div><p>验证配置：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cat memory.limit_in_bytes
<span style=color:#ae81ff>50003968</span>
</code></pre></div><p>注意，读到的值通常是内核页大小的倍数(page size, 4096bytes 或 4KB)。<br>
执行应用程序：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sh ~/test.sh &amp;
</code></pre></div><p>使用该进程PID，将其添加到<code>memory</code>控制器管理下，</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ echo <span style=color:#ae81ff>2845</span> &gt; /sys/fs/cgroup/memory/foo/cgroup.procs
</code></pre></div><p>使用相同的 PID 号，列出正在运行的进程，并验证它是否在期望的 cgroup 中运行：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ ps -o cgroup <span style=color:#ae81ff>2845</span>
CGROUP
8:memory:/foo,1:name<span style=color:#f92672>=</span>systemd:/user.slice/user-0.slice/
↪session-4.scope
</code></pre></div><p>您还可以通过读取指定的文件来监控该 cgroup 当前使用的资源。在这个例子中，你可能想看一下当前进程(以及派生的子进程)的内存使用量。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ cat /sys/fs/cgroup/memory/foo/memory.usage_in_bytes
<span style=color:#ae81ff>253952</span>
</code></pre></div><h2 id=当程序不良运行>当程序不良运行</h2>
<p>还是上面的例子，我们将<code>cgroup/foo</code>内存限制调整为 500 bytes。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ echo <span style=color:#ae81ff>500</span> | sudo tee /sys/fs/cgroup/memory/foo/
↪memory.limit_in_bytes
</code></pre></div><p><em>注意：如果一个任务超出了其定义的限制，内核将进行干预，在某些情况下，会终止该任务。</em><br>
同样，再读这个值，因为它要是内核页大小的倍数。所以尽管你配置的是500字节，但实际上设置的是4KB。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ cat /sys/fs/cgroup/memory/foo/memory.limit_in_bytes
<span style=color:#ae81ff>4096</span>
</code></pre></div><p>启动应用，将其移动到cgroup中，并监控系统日志。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo tail -f /var/log/messages

Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: sh invoked oom-killer:
 ↪gfp_mask<span style=color:#f92672>=</span>0xd0, order<span style=color:#f92672>=</span>0, oom_score_adj<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: sh cpuset<span style=color:#f92672>=</span>/ mems_allowed<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: CPU: <span style=color:#ae81ff>0</span> PID: <span style=color:#ae81ff>2687</span> Comm:
 ↪sh Tainted: G
OE  ------------   3.10.0-327.36.3.el7.x86_64 <span style=color:#75715e>#1</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Hardware name: innotek GmbH
VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: ffff880036ea5c00
 ↪0000000093314010 ffff88000002bcd0 ffffffff81636431
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: ffff88000002bd60
 ↪ffffffff816313cc 01018800000000d0 ffff88000002bd68
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: ffffffffbc35e040
 ↪fffeefff00000000 <span style=color:#ae81ff>0000000000000001</span> ffff880036ea6103
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Call Trace:
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff81636431&gt;<span style=color:#f92672>]</span>
 ↪dump_stack+0x19/0x1b
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff816313cc&gt;<span style=color:#f92672>]</span>
 ↪dump_header+0x8e/0x214
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff8116d21e&gt;<span style=color:#f92672>]</span>
 ↪oom_kill_process+0x24e/0x3b0
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff81088e4e&gt;<span style=color:#f92672>]</span> ?
 ↪has_capability_noaudit+0x1e/0x30
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff811d4285&gt;<span style=color:#f92672>]</span>
 ↪mem_cgroup_oom_synchronize+0x575/0x5a0
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff811d3650&gt;<span style=color:#f92672>]</span> ?
 ↪mem_cgroup_charge_common+0xc0/0xc0
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff8116da94&gt;<span style=color:#f92672>]</span>
 ↪pagefault_out_of_memory+0x14/0x90
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff8162f815&gt;<span style=color:#f92672>]</span>
 ↪mm_fault_error+0x68/0x12b
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff816422d2&gt;<span style=color:#f92672>]</span>
 ↪__do_page_fault+0x3e2/0x450
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff81642363&gt;<span style=color:#f92672>]</span>
 ↪do_page_fault+0x23/0x80
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span>&lt;ffffffff8163e648&gt;<span style=color:#f92672>]</span>
 ↪page_fault+0x28/0x30
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Task in /foo killed as
 ↪a result of limit of /foo
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: memory: usage 4kB, limit
 ↪4kB, failcnt <span style=color:#ae81ff>8</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: memory+swap: usage 4kB,
 ↪limit 9007199254740991kB, failcnt <span style=color:#ae81ff>0</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: kmem: usage 0kB, limit
 ↪9007199254740991kB, failcnt <span style=color:#ae81ff>0</span>
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Memory cgroup stats <span style=color:#66d9ef>for</span> /foo:
 ↪cache:0KB rss:4KB rss_huge:0KB mapped_file:0KB swap:0KB
 ↪inactive_anon:0KB active_anon:0KB inactive_file:0KB
 ↪active_file:0KB unevictable:0KB
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span> pid <span style=color:#f92672>]</span>   uid  tgid total_vm
 ↪rss nr_ptes swapents oom_score_adj name
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span> 2687<span style=color:#f92672>]</span>     <span style=color:#ae81ff>0</span>  <span style=color:#ae81ff>2687</span>    <span style=color:#ae81ff>28281</span>
 ↪347     <span style=color:#ae81ff>12</span>        <span style=color:#ae81ff>0</span>             <span style=color:#ae81ff>0</span> sh
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: <span style=color:#f92672>[</span> 2702<span style=color:#f92672>]</span>     <span style=color:#ae81ff>0</span>  <span style=color:#ae81ff>2702</span>    <span style=color:#ae81ff>28281</span>
 ↪50    <span style=color:#ae81ff>7</span>        <span style=color:#ae81ff>0</span>             <span style=color:#ae81ff>0</span> sh
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Memory cgroup out of memory:
 ↪Kill process <span style=color:#ae81ff>2687</span> <span style=color:#f92672>(</span>sh<span style=color:#f92672>)</span> score <span style=color:#ae81ff>0</span> or sacrifice child
Oct <span style=color:#ae81ff>14</span> 10:22:40 localhost kernel: Killed process <span style=color:#ae81ff>2702</span> <span style=color:#f92672>(</span>sh<span style=color:#f92672>)</span>
 ↪total-vm:113124kB, anon-rss:200kB, file-rss:0kB
Oct <span style=color:#ae81ff>14</span> 10:22:41 localhost kernel: sh invoked oom-killer:
 ↪gfp_mask<span style=color:#f92672>=</span>0xd0, order<span style=color:#f92672>=</span>0, oom_score_adj<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
<span style=color:#f92672>[</span> ... <span style=color:#f92672>]</span>
</code></pre></div><p>注意，一旦应用程序使用内存达到 4KB 限制，内核的 Out-Of-Memory Killer（或 oom-killer）就会介入。它杀死了应用程序。您可以下面的方式来验证这一点：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ ps -o cgroup <span style=color:#ae81ff>2687</span>
CGROUP
</code></pre></div><h2 id=使用-libcgroup>使用 libcgroup</h2>
<p><code>libcgroup</code>软件包提供了简单的管理工具，上面很多操作步骤都可以用它实现。例如，使用cgcreate命令可以创建sysfs条目和文件。<br>
在<code>memory</code>子系统下创建名字为foo的组，使用下面命令：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cgcreate -g memory:foo
</code></pre></div><p><em>注意：libcgroup 提供了一种用于管理<strong>控制组</strong>中的任务的机制。</em><br>
使用与之前相同的方法，设置阈值：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ echo <span style=color:#ae81ff>50000000</span> | sudo tee
 ↪/sys/fs/cgroup/memory/foo/memory.limit_in_bytes
</code></pre></div><p>验证配置：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cat memory.limit_in_bytes
<span style=color:#ae81ff>50003968</span>
</code></pre></div><p>使用 cgexec 命令在 <code>cgroup/foo</code> 下运行应用程序：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cgexec -g memory:foo ~/test.sh
</code></pre></div><p>使用它的 PID，验证应用程序是否在 cgroup 和定义的<code>memory</code>管理器下运行：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$  ps -o cgroup <span style=color:#ae81ff>2945</span>
CGROUP
6:memory:/foo,1:name<span style=color:#f92672>=</span>systemd:/user.slice/user-0.slice/
↪session-1.scope
</code></pre></div><p>如果您的应用程序不再运行，并且您想要清理并删除 cgroup，您可以使用 cgdelete。要从<code>memory</code>控制器下删除组 foo，请键入：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cgdelete memory:foo
</code></pre></div><h2 id=持久组>持久组</h2>
<p>通过简单的配置文件来启动服务，也可以完成上面的工作。你可以在<code>/etc/cgconfig.conf</code>文件中定义所有cgroup名字和属性。下面的例子中配置了foo组和它的一些属性。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ cat /etc/cgconfig.conf
<span style=color:#75715e>#</span>
<span style=color:#75715e>#  Copyright IBM Corporation. 2007</span>
<span style=color:#75715e>#</span>
<span style=color:#75715e>#  Authors:     Balbir Singh &lt;balbir@linux.vnet.ibm.com&gt;</span>
<span style=color:#75715e>#  This program is free software; you can redistribute it</span>
<span style=color:#75715e>#  and/or modify it under the terms of version 2.1 of the GNU</span>
<span style=color:#75715e>#  Lesser General Public License as published by the Free</span>
<span style=color:#75715e>#  Software Foundation.</span>
<span style=color:#75715e>#</span>
<span style=color:#75715e>#  This program is distributed in the hope that it would be</span>
<span style=color:#75715e>#  useful, but WITHOUT ANY WARRANTY; without even the implied</span>
<span style=color:#75715e>#  warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR</span>
<span style=color:#75715e>#  PURPOSE.</span>
<span style=color:#75715e>#</span>
<span style=color:#75715e># 默认，我们希望 systemd 默认加载所有内容</span>
<span style=color:#75715e># 所以没啥可做的</span>
<span style=color:#75715e># 详细内容查看 man cgconfig.conf</span>
<span style=color:#75715e># 了解如何在系统启动时使用该文件创建 cgroup</span>

group foo <span style=color:#f92672>{</span>
  cpu <span style=color:#f92672>{</span>
    cpu.shares <span style=color:#f92672>=</span> 100;
  <span style=color:#f92672>}</span>
  memory <span style=color:#f92672>{</span>
    memory.limit_in_bytes <span style=color:#f92672>=</span> 5000000;
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><p><code>cpu.shares</code>定义了cgroup的CPU优先级。默认，所有的组继承 1024 shares 或者说 100% CPU使用时间。降低该值，比如 100，该组将被限制在大约 10% CPU使用时间。<br>
如前所述，cgroup 中的进程也可以被限制使用CPUs(core)数量，把下面的内容添加到cgconfig.conf文件相应的cgroup下：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cpuset <span style=color:#f92672>{</span>
  cpuset.cpus<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;0-5&#34;</span>;
<span style=color:#f92672>}</span>
</code></pre></div><p>它将限制该cgroup使用索引为0到5的核心(core)，即仅能使用前6个CPU核心。<br>
下面，需要使用<code>cgconfig</code>服务加载该配置文件。首先，配置<code>cgconfig</code>开机自启动加载上面的配置文件。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo systemctl enable cgconfig
Create symlink from /etc/systemd/system/sysinit.target.wants/
↪cgconfig.service
to /usr/lib/systemd/system/cgconfig.service.
</code></pre></div><p>现在，手动启动服务加载配置文件（或者直接重启操作系统）</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo systemctl start cgconfig
</code></pre></div><p>在<code>cgroup/foo</code>下，启动应用，并将其和它的<code>memory</code>、<code>cpuset</code>和<code>cpu</code>限制进行绑定：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo cgexec -g memory,cpu,cpuset:foo ~/test.sh &amp;
</code></pre></div><p>除了将应用启动到预定义的cgroup中之外，剩下的操作系统重启后会一直存在。但是，你可以通过写一个依赖<code>cgconfig</code>服务的开机初始化脚本来启动应用。</p>
</div>
<div class="col-12 col-md-6 mb-2"><p>Apache APISIX - 专门为 kubernetes 研发的入口控制器。</p>
<h2 id=状态>状态</h2>
<p>该项目目前是 general availability 级别</p>
<h2 id=先决条件>先决条件</h2>
<p>apisix-ingress-controller 要求 kubernetes 版本 1.16+. 因为使用了 CustomResourceDefinition v1 stable 版本的 API.
从 1.0.0 版本开始，APISIX-ingress-controller 要求 Apache APISIX 版本 2.7+.</p>
<h2 id=功能特性>功能特性</h2>
<ul>
<li>使用 Custom Resource Definitions(CRDs) 对 Apache APISIX 进行声明式配置，使用 kubernetes yaml 结构最小化学习成本。</li>
<li>Yaml 配置热加载</li>
<li>支持原生 Kubernetes Ingress (v1 和 v1beta1) 资源</li>
<li>Kubernetes endpoint 自动注册到 Apache APISIX 上游节点</li>
<li>支持基于 POD（上游节点） 的负载均衡</li>
<li>开箱支持上游节点健康检查</li>
<li>扩展插件支持热配置并且立即生效</li>
<li>支持路由的 SSL 和 mTLS</li>
<li>支持流量切割和金丝雀发布</li>
<li>支持 TCP 4层代理</li>
<li>Ingress 控制器本身也是一个可插拔的热加载组件</li>
<li>多集群配置分发</li>
</ul>
<p><a href="https://docs.google.com/spreadsheets/d/191WWNpjJ2za6-nbG4ZoUMXMpUK8KlCIosvQB0f-oq3k/edit#gid=907731238">这里有一份在线竞品分析表格</a></p>
<h3 id=apache-apisix-ingress-vs-kubernetes-nginx-ingress>Apache APISIX Ingress vs. Kubernetes Nginx Ingress</h3>
<ul>
<li>yaml 配置热加载</li>
<li>更方便的金丝雀发布</li>
<li>配置验证，安全可靠</li>
<li>丰富的插件和生态, <a href=https://github.com/apache/apisix/tree/master/docs/en/latest/plugins>插件列表</a></li>
<li>支持 APISIX 自定义资源和原生 kubernetes ingress 资源</li>
<li>更活跃的社区</li>
</ul>
<h2 id=设计原理>设计原理</h2>
<h3 id=架构>架构</h3>
<p>apisix-ingress-controller 需要的所有配置都是通过 Kubernetes CRDs (Custom Resource Definitions) 定义的。支持在 Apache APISIX 中配置插件、上游的服务注册发现机制、负载均衡等。<br>
apisix-ingress-controller 是 Apache APISIX 的控制面组件. 当前服务于 Kubernetes 集群。 未来, 计划分离出子模块以适配更多的部署模式，比如虚拟机集群部署。</p>
<p>整体架构图如下：</p>
<p><img src=images/apisix-ingress-controller-design/arch.png alt=architecture></p>
<p>这是一张内部架构图：</p>
<p><img src=images/apisix-ingress-controller-design/internal-arch.png alt=internal-arch></p>
<h3 id=时序流程图>时序/流程图</h3>
<p>apisix-ingress-controller 负责和 Kubernetes Apiserver 交互, 申请可访问资源权限（RBAC），监控变化，在 Ingress 控制器中实现对象转换，比较变化，然后同步到 Apache APISIX。</p>
<p><img src=images/apisix-ingress-controller-design/flow.png alt=flow></p>
<p>这是一张流程图，介绍了ApisixRoute和其他CRD在同步过程中的主要逻辑</p>
<p><img src=images/apisix-ingress-controller-design/sync-logic-controller.png alt=sync-logic-controller></p>
<h3 id=结构转换>结构转换</h3>
<p>apisix-ingress-controller 给 CRDs 提供了外部配置方法。它旨在务于需要日常操作和维护的运维人员，他们需要经常处理大量路由配置，希望在一个配置文件中处理所有相关的服务，同时还希望能具备便捷和易于理解的管理能力。但是，Apache APISIX 则是从网关的角度设计的，并且所有的路由都是独立的。这就导致了两者在数据结构上存在差异。一个注重批量定义，一个注重离散实现。<br>
考虑到不同人群的使用习惯，CRDs 的数据结构借鉴了 Kubernetes Ingress 的数据结构，数据结构基本一致。
关于这两者的差别，请看下面这张图：</p>
<p><img src=images/apisix-ingress-controller-design/struct-compare.png alt=struct-compare></p>
<p>可以看到，它们是多对多的关系。因此，apisix-ingress-controller 必须对 CRD 做一些转换，以适应不同的网关。</p>
<h3 id=规则比较>规则比较</h3>
<p>seven 模块内部保存了内存数据结构，目前与Apache APISIX资源对象非常相似。当 Kubernetes 资源对象有新变化时，seven 会比较内存对象，并根据比较结果进行增量更新。<br>
目前的比较规则是根据route/service/upstream资源对象的分组，分别进行比较，发现差异后做出相应的广播通知。</p>
<p><img src=images/apisix-ingress-controller-design/diff-rules.png alt=diff-rules></p>
<h3 id=服务发现>服务发现</h3>
<p>根据 <code>ApisixUpstream</code> 中定义的 <code>namespace</code> <code>name</code> <code>port</code> 字段，apisix-ingress-controller 会在 Apache APISIX Upstream 中注册 处于 running 状态的 endpoints 节点，并且根据 kubernetes endpoints 状态进行实时同步。<br>
基于服务发现，apisix-ingress-controller 可以直接访问后端 pod，绕过 Kubernetes Service，可以实现自定义的负载均衡策略。</p>
<h3 id=annotation-实现>Annotation 实现</h3>
<p>不像 Kubernetes Nginx Ingress Controller，apisix-ingress-controller 的 annotation 实现是基于 Apache APISIX 的插件机制的。<br>
比如，可以通过在<code>ApisixRoute</code>资源对象中设置<code>k8s.apisix.apache.org/whitelist-source-range</code>annotation来配置白名单。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>annotations</span>:
    <span style=color:#f92672>k8s.apisix.apache.org/whitelist-source-range</span>: <span style=color:#ae81ff>1.2.3.4</span>,<span style=color:#ae81ff>2.2.0.0</span><span style=color:#ae81ff>/16</span>
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpserver-route</span>
<span style=color:#f92672>spec</span>:
    <span style=color:#ae81ff>...</span>
</code></pre></div><p>黑/白名单功能是通过<a href=https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/ip-restriction.md>ip-restriction</a>插件来实现的。<br>
为方便的定义一些常用的配置，未来会有更多的 annotation 实现，比如CORS。</p>
<h2 id=apisixroute-介绍>ApisixRoute 介绍</h2>
<p>ApisixRoute 是一个 CRD 资源，它关注如何将流量发送到后端，它有很多 APISIX 支持的特性。相比 Ingress，功能实现的更原生，语意更强。</p>
<h3 id=基于路径的路由规则>基于路径的路由规则</h3>
<p>URI 路径总是用于拆分流量，比如访问 foo.com 的请求， 含有 /foo 前缀请求路由到 foo 服务，访问 /bar 的请求要路由到 bar 服务。以 ApisixRoute 方式配置应该是这样的：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>foo-bar-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>foo</span>
    <span style=color:#f92672>match</span>:
      <span style=color:#f92672>hosts</span>:
      - <span style=color:#ae81ff>foo.com</span>
      <span style=color:#f92672>paths</span>:
      - <span style=color:#e6db74>&#34;/foo*&#34;</span>
    <span style=color:#f92672>backends</span>:
     - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
       <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>bar</span>
    <span style=color:#f92672>match</span>:
      <span style=color:#f92672>paths</span>:
        - <span style=color:#e6db74>&#34;/bar&#34;</span>
    <span style=color:#f92672>backends</span>:
      - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>bar</span>
        <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
</code></pre></div><p>有<code>prefix</code>和<code>exact</code>两种路径类型可用 默认<code>exact</code>，当需要前缀匹配的时候，就在路径后加 * 比如 /id/* 能匹配所有带 /id/ 前缀的请求。</p>
<h3 id=高级路由特性>高级路由特性</h3>
<p>基于路径的路由是最普遍的，但这并不够， 再试一下其他路由方式，比如 <code>methods</code> 和 <code>exprs</code></p>
<p><code>methods</code> 通过 HTTP 动作来切分流量，下面例子会把所有 GET 请求路由到 foo 服务（kubernetes service）</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/</span>
        <span style=color:#f92672>methods</span>:
          - <span style=color:#ae81ff>GET</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
</code></pre></div><p><code>exprs</code>允许用户使用 HTTP 中的任意字符串来配置匹配条件，例如query、HTTP Header、Cookie。它可以配置多个表达式，而这些表达式又由主题(subject)、运算符(operator)和值/集合(value/set)组成。比如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/</span>
        <span style=color:#f92672>exprs</span>:
          - <span style=color:#f92672>subject</span>:
              <span style=color:#f92672>scope</span>: <span style=color:#ae81ff>Query</span>
              <span style=color:#f92672>name</span>: <span style=color:#ae81ff>id</span>
            <span style=color:#f92672>op</span>: <span style=color:#ae81ff>Equal</span>
            <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;2143&#34;</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
</code></pre></div><p>上面是绝对匹配，匹配所有请求的 query 字符串中 id 的值必须等于 2143。</p>
<h4 id=服务解析粒度>服务解析粒度</h4>
<p>默认，apisix-ingress-controller 会监听 service 的引用，所以最新的 endpoints 列表会被更新到 Apache APISIX。同样 apisix-ingress-controller 也可以直接使用 service 自身的 clusterIP。如果这正是你想要的，配置 <code>resolveGranularity: service</code>(默认<code>endpoint</code>). 如下：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/*</span>
        <span style=color:#f92672>methods</span>:
          - <span style=color:#ae81ff>GET</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
          <span style=color:#f92672>resolveGranularity</span>: <span style=color:#ae81ff>service</span>
</code></pre></div><h3 id=基于权重的流量切分>基于权重的流量切分</h3>
<p>这是 APISIX Ingress Controller 一个非常棒的特性。一个路由规则中可以指定多个后端，当多个后端共存时，将应用基于权重的流量拆分（实际上是使用Apache APISIX中的流量拆分 <a href=https://apisix.apache.org/zh/docs/apisix/plugins/traffic-split/>traffic-split</a> 插件）您可以为每个后端指定权重，默认权重为 100。比如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>method</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/*</span>
        <span style=color:#f92672>methods</span>:
          - <span style=color:#ae81ff>GET</span>
        <span style=color:#f92672>exprs</span>:
          - <span style=color:#f92672>subject</span>:
              <span style=color:#f92672>scope</span>: <span style=color:#ae81ff>Header</span>
              <span style=color:#f92672>name</span>: <span style=color:#ae81ff>User-Agent</span>
            <span style=color:#f92672>op</span>: <span style=color:#ae81ff>RegexMatch</span>
            <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;.*Chrome.*&#34;</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
          <span style=color:#f92672>weight</span>: <span style=color:#ae81ff>100</span>
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>bar</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>81</span>
          <span style=color:#f92672>weight</span>: <span style=color:#ae81ff>50</span>
</code></pre></div><p>上面有一个路由规则（1.所有<code>GET /*</code>请求 2.Header中有匹配 <code>User-Agent: .*Chrome.*</code> 的条目）它有两个后端服务 foo、bar，权重是100：50，意味着有2/3的流量会进入 foo，有1/3的流量会进入bar。</p>
<h3 id=插件>插件</h3>
<p>Apache APISIX 提供了 40 多个插件，可以在 APIsixRoute 中使用。所有配置项的名称与 APISIX 中的相同。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>hosts</span>:
        - <span style=color:#ae81ff>local.httpbin.org</span>
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/*</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>foo</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>80</span>
      <span style=color:#f92672>plugins</span>:
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cors</span>
          <span style=color:#f92672>enable</span>: <span style=color:#66d9ef>true</span>
</code></pre></div><p>为到 local.httpbin.org 的请求都配置了 Cors 插件</p>
<h3 id=websocket-代理>Websocket 代理</h3>
<p>创建一个 route，配置特定的 websocket 字段，就可以代理 websocket 服务。比如：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ws-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>http</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>websocket</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>hosts</span>:
          - <span style=color:#ae81ff>ws.foo.org</span>
        <span style=color:#f92672>paths</span>:
          - <span style=color:#ae81ff>/*</span>
      <span style=color:#f92672>backends</span>:
        - <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>websocket-server</span>
          <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>8080</span>
      <span style=color:#f92672>websocket</span>: <span style=color:#66d9ef>true</span>
</code></pre></div><h3 id=tcp-路由>TCP 路由</h3>
<p>apisix-ingress-controller 支持基于端口的 tcp 路由</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tcp-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>stream</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tcp-route-rule1</span>
      <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>ingressPort</span>: <span style=color:#ae81ff>9100</span>
      <span style=color:#f92672>backend</span>:
        <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>tcp-server</span>
        <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>8080</span>
</code></pre></div><p>进入 apisix-ingress-controller 9100 端口的 TCP 流量会路由到后端 tcp-server 服务。</p>
<p><strong>注意：</strong> APISIX不支持动态监听，所以需要在APISIX<a href=https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L111>配置文件</a>中预先定义9100端口。</p>
<h3 id=udp-路由>UDP 路由</h3>
<p>apisix-ingress-controller 支持基于端口的 udp 路由</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v2beta3</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixRoute</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>udp-route</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>stream</span>:
    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>udp-route-rule1</span>
      <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>UDP</span>
      <span style=color:#f92672>match</span>:
        <span style=color:#f92672>ingressPort</span>: <span style=color:#ae81ff>9200</span>
      <span style=color:#f92672>backend</span>:
        <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>udp-server</span>
        <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>53</span>
</code></pre></div><p>进入 apisix-ingress-controller 9200 端口的 TCP 流量会路由到后端 udp-server 服务。</p>
<p><strong>注意：</strong> APISIX不支持动态监听，所以需要在APISIX<a href=https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L111>配置文件</a>中预先定义9200端口。</p>
<h2 id=apisixupstream-介绍>ApisixUpstream 介绍</h2>
<p>ApisixUpstream 是 kubernetes service 的装饰器。它设计成与其关联的 kubernetes service 的名字一致，将其变得更加强大，使该 kubernetes service 能够配置负载均衡策略、健康检查、重试、超时参数等。<br>
通过 ApisixUpstream 和 kubernetes service，apisix-ingress-controller 会生成 APISIX Upstream(s).</p>
<h3 id=配置负载均衡>配置负载均衡</h3>
<p>需要适当的负载均衡算法来合理地分散 Kubernetes Service 的请求</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>loadbalancer</span>:
    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ewma</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>httpbin</span>
  <span style=color:#f92672>ports</span>:
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>http</span>
    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</code></pre></div><p>上面这个例子给 httpbin 服务配置了 ewma 负载均衡算法。有时候可能会需要会话保持，你可以配置一致性哈希负载均衡算法</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>loadbalancer</span>:
    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>chash</span>
    <span style=color:#f92672>hashOn</span>: <span style=color:#ae81ff>header</span>
    <span style=color:#f92672>key</span>: <span style=color:#e6db74>&#34;user-agent&#34;</span>
</code></pre></div><p>这样 apisix 就会根据 user-agent header 来分发流量。</p>
<h3 id=配置健康检查>配置健康检查</h3>
<p>尽管 kubelet 已经提供了检测 pod 健康的探针机制。你可能还需要更加丰富的健康检查机制，比如被动健康检查机制。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>healthCheck</span>:
    <span style=color:#f92672>passive</span>:
      <span style=color:#f92672>unhealthy</span>:
        <span style=color:#f92672>httpCodes</span>:
          - <span style=color:#ae81ff>500</span>
          - <span style=color:#ae81ff>502</span>
          - <span style=color:#ae81ff>503</span>
          - <span style=color:#ae81ff>504</span>
        <span style=color:#f92672>httpFailures</span>: <span style=color:#ae81ff>3</span>
        <span style=color:#f92672>timeout</span>: <span style=color:#ae81ff>5s</span>
    <span style=color:#f92672>active</span>:
      <span style=color:#f92672>type</span>: <span style=color:#ae81ff>http</span>
      <span style=color:#f92672>httpPath</span>: <span style=color:#ae81ff>/healthz</span>
      <span style=color:#f92672>timeout</span>: <span style=color:#ae81ff>5s</span>
      <span style=color:#f92672>host</span>: <span style=color:#ae81ff>www.foo.com</span>
      <span style=color:#f92672>healthy</span>:
        <span style=color:#f92672>successes</span>: <span style=color:#ae81ff>3</span>
        <span style=color:#f92672>interval</span>: <span style=color:#ae81ff>2s</span>
        <span style=color:#f92672>httpCodes</span>:
          - <span style=color:#ae81ff>200</span>
          - <span style=color:#ae81ff>206</span>
</code></pre></div><p>上面的yaml片段定义了被动健康检查器来检查endpoints的健康状况。一旦连续三次请求的响应状态码是错误（500 502 503 504 中的一个），这个endpoint就会被标记成不健康并不会再给它分配流量了，直到它再次健康。<br>
所以，主动健康检查器就出现了。endpoint 可能掉线一段时间又回复健康，主动健康检查器主动探测这些不健康的endpoints，一旦满足健康条件就将其恢复为健康（条件：连续三次请求响应状态码为200或206）</p>
<blockquote>
<p>注意：主动健康检查器在某种程度上与 liveness/readiness 探针重复，但如果使用被动健康检查机制，则它是必需的。因此，一旦您使用了 ApisixUpstream 中的健康检查功能，主动健康检查器是强制性的。</p>
</blockquote>
<h3 id=配置重试和超时>配置重试和超时</h3>
<p>当请求出现错误，比如网络问题或者服务不可用当时候，你可能想重试请求。默认重试次数是1，通过定义<code>retries</code>字段可以改变这个值。<br>
下面这个例子将<code>retries</code>定义为3，表明会对kubernetes service/httpbin的endpoints最多请求3次。</p>
<blockquote>
<p>注意：只有在尚未向客户端响应任何内容的情况下，才有可能将请求重试传递到下一个端点。也就是说，如果在传输响应的过程中发生错误或超时，就不会重试了。</p>
</blockquote>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>retries</span>: <span style=color:#ae81ff>3</span>
</code></pre></div><p>默认，connect、send 和 read 的超时时间是60s，这可能对有些应用不合适，修改<code>timeout</code>字段来改变默认值。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>httpbin</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>timeout</span>:
    <span style=color:#f92672>connect</span>: <span style=color:#ae81ff>5s</span>
    <span style=color:#f92672>read</span>: <span style=color:#ae81ff>10s</span>
    <span style=color:#f92672>send</span>: <span style=color:#ae81ff>10s</span>
</code></pre></div><p>上面例子将connect、read 和 send 分别设置为 5s、10s、10s。</p>
<h3 id=端口级别配置>端口级别配置</h3>
<p>有时，单个 kubernetes service 可能会暴露多个端口，这些端口提供不同的功能并且需要不同的上游配置。在这种情况下，您可以为单个端口创建配置。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apisix.apache.org/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ApisixUpstream</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>foo</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>loadbalancer</span>:
    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>roundrobin</span>
  <span style=color:#f92672>portLevelSettings</span>:
  - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>7000</span>
    <span style=color:#f92672>scheme</span>: <span style=color:#ae81ff>http</span>
  - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>7001</span>
    <span style=color:#f92672>scheme</span>: <span style=color:#ae81ff>grpc</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>foo</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>foo</span>
  <span style=color:#f92672>portLevelSettings</span>:
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>http</span>
    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>7000</span>
    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>7000</span>
  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>grpc</span>
    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>7001</span>
    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>7001</span>
</code></pre></div><p>foo 服务暴露的两个端口，一个使用http协议，另一个使用grpc协议。同时，ApisixUpstream/foo 为7000端口配置http协议，为7001端口配置grpc协议（所有端口都是service端口），两个端口都共享使用同一个负载均衡算法。<br>
如果服务仅公开一个端口，则 PortLevelSettings 不是必需的，但在定义多个端口时很有用。</p>
<h2 id=用户故事>用户故事</h2>
<p><a href=https://mp.weixin.qq.com/s/bmm2ibk2V7-XYneLo9XAPQ>思必驰：为什么我们重新写了一个 k8s ingress controller？</a><br>
<a href=https://www.upyun.com/opentalk/448.html>腾讯云：为什么选择 apisix 实现 kubernetes ingress controller</a></p>
</div>
<div class="col-12 col-md-6 mb-2"><h2 id=ppt-分享>PPT 分享</h2>
<p>以下是 &lt;实践中总结 Kubernetes 必须了解的核心内容> 主题分享 PPT</p>
<p><img src=images/intro-kubernetes/1.jpg alt=ppt-page-1></p>
<hr>
<p><img src=images/intro-kubernetes/3.jpg alt=ppt-page-3></p>
<hr>
<p><img src=images/intro-kubernetes/4.jpg alt=ppt-page-4></p>
<hr>
<p><img src=images/intro-kubernetes/5.jpg alt=ppt-page-5></p>
<hr>
<p><img src=images/intro-kubernetes/6.jpg alt=ppt-page-6></p>
<hr>
<p><img src=images/intro-kubernetes/7.jpg alt=ppt-page-7></p>
<hr>
<p><img src=images/intro-kubernetes/8.jpg alt=ppt-page-8></p>
<hr>
<p><img src=images/intro-kubernetes/9.jpg alt=ppt-page-9></p>
<hr>
<p><img src=images/intro-kubernetes/10.jpg alt=ppt-page-10></p>
<hr>
<p><img src=images/intro-kubernetes/11.jpg alt=ppt-page-11></p>
<hr>
<p><img src=images/intro-kubernetes/12.jpg alt=ppt-page-12></p>
<hr>
<p><img src=images/intro-kubernetes/13.jpg alt=ppt-page-13></p>
<hr>
<p><img src=images/intro-kubernetes/14.jpg alt=ppt-page-14></p>
<hr>
<p><img src=images/intro-kubernetes/15.jpg alt=ppt-page-15></p>
<hr>
<p><img src=images/intro-kubernetes/16.jpg alt=ppt-page-16></p>
<hr>
<p><img src=images/intro-kubernetes/17.jpg alt=ppt-page-17></p>
<hr>
<p><img src=images/intro-kubernetes/18.jpg alt=ppt-page-18></p>
<hr>
<p><img src=images/intro-kubernetes/19.jpg alt=ppt-page-19></p>
<hr>
<p><img src=images/intro-kubernetes/20.jpg alt=ppt-page-20></p>
<hr>
<p><img src=images/intro-kubernetes/21.jpg alt=ppt-page-21></p>
<hr>
<p><img src=images/intro-kubernetes/22.jpg alt=ppt-page-22></p>
<hr>
<p><img src=images/intro-kubernetes/23.jpg alt=ppt-page-23></p>
<hr>
<p><img src=images/intro-kubernetes/24.jpg alt=ppt-page-24></p>
<hr>
<p><img src=images/intro-kubernetes/25.jpg alt=ppt-page-25></p>
<hr>
<p><img src=images/intro-kubernetes/26.jpg alt=ppt-page-26></p>
<hr>
<p><img src=images/intro-kubernetes/27.jpg alt=ppt-page-27></p>
<hr>
<p><img src=images/intro-kubernetes/28.jpg alt=ppt-page-28></p>
<hr>
<p><img src=images/intro-kubernetes/29.jpg alt=ppt-page-29></p>
<hr>
<p><img src=images/intro-kubernetes/30.jpg alt=ppt-page-30></p>
<hr>
<p><img src=images/intro-kubernetes/31.jpg alt=ppt-page-31></p>
<hr>
<p><img src=images/intro-kubernetes/32.jpg alt=ppt-page-32></p>
<hr>
<p><img src=images/intro-kubernetes/33.jpg alt=ppt-page-33></p>
<hr>
<p><img src=images/intro-kubernetes/34.jpg alt=ppt-page-34></p>
<hr>
<p><img src=images/intro-kubernetes/35.jpg alt=ppt-page-35></p>
<hr>
<p><img src=images/intro-kubernetes/36.jpg alt=ppt-page-36></p>
<hr>
<p><img src=images/intro-kubernetes/37.jpg alt=ppt-page-37></p>
<hr>
<p><img src=images/intro-kubernetes/38.jpg alt=ppt-page-38></p>
<hr>
<p><img src=images/intro-kubernetes/39.jpg alt=ppt-page-39></p>
<hr>
<p>完~</p>
</div>
<div class="col-12 col-md-6 mb-2"><h2 id=前言>前言</h2>
<p>kubernetes 中 pod 的设计是一个伟大的发明, 今天我很有必要去聊一下 pod 和 container, 探究一下它们究竟是什么? kubernetes 官方文档中关于<a href=https://kubernetes.io/zh/docs/concepts/workloads/pods/#pod-storage>pod 概念介绍</a>提供了一个完整的解释, 但写的不够详细, 表达过于专业, 但还是很推荐大家阅读一下. 当然这篇文档应该更接地气.</p>
<h2 id=容器真的存在吗>容器真的存在吗?</h2>
<p>linux 中是没有容器这个概念的, 容器就是 linux 中的普通进程, 它使用了 linux 内核提供的两个重要的特性: namespace & cgroups.</p>
<p>namespace 提供了一种隔离的特性, 让它之外的内容隐藏, 给它下面的进程一个不被干扰的运行环境(其实不完全,下面说) .</p>
<p>namespace 包含:</p>
<ul>
<li>hostname</li>
<li>Process IDs</li>
<li>File System</li>
<li>Network Interface</li>
<li>Inter-Process Communication(IPC)</li>
</ul>
<p>接上面, 其实 namespace 内部的进程并不是完全不和外面的进程产生影响的. 进程可以不受限制的使用物理机上的所有资源, 这样就会导致其他进程无资源可用. 所以, 为了限制进程资源使用, linux 提供了另一种特性 cgroups. 进程可以像在 namespace 中运行, 但是 cgroups 限制了进程的可以使用的资源. 这些资源包括:</p>
<ul>
<li>CPU</li>
<li>RAM</li>
<li>block I/O</li>
<li>network I/O</li>
<li>etc.</li>
</ul>
<p>CPU 通常按照毫核来限制(单位:m), 1000m=1C; 内存按照RAM的字节数来限制. 进程可以在 cgroups 设置的资源限制范围内运行, 不允许超限使用, 比如, 超过内存限制就会报 OOM(out of memory) 的错误.</p>
<p>需要特别说明的是, 上面提到的 namespace & cgroup 都是 Linux 独立的特性, 你可以使用上面提到的 namespace 中的一个或者多个. namespace & cgroup 作用到一组或者一个进程上. 你可以把多个进程放在一个 namespace 中, 这样它们就可以彼此交互, 或者 把他们放在一个 cgroups 中, 这样他们就可以共享一个CPU & Mem 资源限制.</p>
<h2 id=组合容器>组合容器</h2>
<p>我们都用过 docker, 当我们启动一个容器的时候, docker 会帮我们给每一个容器创建它们自己的 namespace & cgroups. 这应该就是我们理解的容器.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809102430362.png alt=image-20210809102424781></p>
<p>如图, 容器本身还是比较独立的, 他们可能会有映射到主机的端口和卷, 这样就可以和外面通信. 但是我们也可以通过一些命令将多个容器组合到一组namespace中, 下面我们举个例子说明:</p>
<p>首先, 创建一个 nginx 容器:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e># cat &lt;&lt;EOF &gt;&gt; nginx.conf</span>
&gt; error_log stderr;
&gt; events <span style=color:#f92672>{</span> worker_connections  1024; <span style=color:#f92672>}</span>
&gt; http <span style=color:#f92672>{</span>
&gt;     access_log /dev/stdout combined;
&gt;     server <span style=color:#f92672>{</span>
&gt;         listen <span style=color:#ae81ff>80</span> default_server;
&gt;         server_name example.com www.example.com;
&gt;         location / <span style=color:#f92672>{</span>
&gt;             proxy_pass http://127.0.0.1:2368;
&gt;         <span style=color:#f92672>}</span>
&gt;     <span style=color:#f92672>}</span>
&gt; <span style=color:#f92672>}</span>
&gt; EOF
<span style=color:#75715e># docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf -p 8080:80 --ipc=shareable nginx </span>
</code></pre></div><p>接着, 我们再启动一个 ghost 容器, ghost 是一个开源的博客系统, 同时我们添加几个额外的命令到 nginx 容器上.</p>
<pre tabindex=0><code># docker run -d --name ghost --net=container:nginx --ipc=container:nginx --pid=container:nginx ghost
</code></pre><p>好了, 现在 nginx 容器可以通过 localhost 将请求代理到 ghost 容器, 访问 <code>http://localhost:8080</code>试试, 你可以通过 nginx 反向代理看到一个 ghost 博客. 上面的命令就把一组容器组合到里同一组 namespace 中, 容器彼此之间可以互相发现/通信.</p>
<p>就像这样:</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809134007587.png alt=image-20210809134007587></p>
<h2 id=某种意义上-pod-就是一组容器>某种意义上, pod 就是一组容器</h2>
<p>现在我们已经知道, 我们可以把一组进程组合到一个 namespace & cgroups 中, 这就是 kubernetes 中的 pod. pod 允许你定义你要运行的容器, 然后 kubernetes 会帮正确的配置 namespace & cgroups. 它稍微复杂的一点是, 网络这块它没用 docker network, 而是用到了 CNI(通用网络接口), 但原理都差不多.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809135653975.png alt=image-20210809135653975></p>
<p>按照上述方式创建的 pod, 更像是运行在同一台机器上, 他们之间可以通过 localhost 通信, 可以共享存储卷. 甚至他们可以使用 IPC 或者互相发送 HUP / TERM 这类信号.</p>
<p>我们再举个例子, 如下图, 我们运行一个 nginx 反向代理 app, 再运行一个 confd, 当 app 实例增加或减少的时候去动态配置 <code>nginx.conf</code> 并重启 nginx, etcd 中存储了 app 的 ip 地址. 当 ip 列表发生变化, confd 会收到 etcd 发的通知, 并更新 <code>nginx.conf</code> 并给 nginx 发送一个 HUP 信号, nginx 收到 HUP 信号会重启.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809141738720.png alt=image-20210809141738720></p>
<p>如果用 docker, 你大概会把 nginx 和 confd 放在一个容器中. 由于 docker 只有一个 entrypoint, 所以你要启动一个类似 supervisord 一样的进程管理器 来让 nginx 和 confd 都运行起来. 你每启动一个 nginx 副本就要启动一个 supervisord, 这不好吧. 更重要的是, docker 只知道 supervisord 的状态, 因为它只有一个 entrypoint. 它看不到里面的所有进程, 这就意味着, 你用 docker 提供的工具获取不到他们的信息. 一旦 nginx <code>Crash-Restart Loop</code>, docker 一点办法没有.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809142718304.png alt=image-20210809142718304></p>
<p>通过 pod , kubernetes 能管理每一个进程, 看到他们的状态, 它可以通过 api 将进程状态信息暴露给用户, 或者提供进程崩溃时重启/记录日志等服务.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809143225671.png alt=image-20210809143225671></p>
<h2 id=把容器当作接口>把容器当作接口</h2>
<p>使用 pod 这种组织容器的方式, 可以把容器当作提供各种功能的 &ldquo;接口&rdquo;. 它不同于传统意义上的 web 接口. 更像是可以被容器所使用的某种抽象意义的接口.</p>
<p>我们拿上面 nginx+confd 的例子来说, confd 不需要知道任何 nginx 进程的东西, 它就只需要去 watch etcd 然后给 nginx 进程发送 HUP 信号或者执行个命令. 而且你可以把 nginx 替换成其他任何类型的应用, 以这样的模式来使用 confd 的这种能力. 这种模式下, confd 通常被称作 <strong>&ldquo;sidecar container&rdquo;</strong> 边车容器, 下面这图就很形象.</p>
<p><img src=images/what-are-kubernetes-pods-anyway.assets/image-20210809150256978.png alt=image-20210809150256978></p>
<p>像 istio 这样的服务网格项目, 也是, 给应用程序容器放置一个边车容器来提供服务路由, 遥测, 网络策略等功能, 但是对应用程序并没有做任何侵略性更改. 你也可以使用多个边车容器来组织 pod, 比如在一个 pod 中同时放置 confd & istio 边车容器. 用这样的方式, 可以构建更加复杂可靠的系统, 同时还能保持每个应用的独立性和简单性.</p>
<h2 id=参考>参考</h2>
<p><a href=https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway>What are Kubernetes Pods Anyway?</a></p>
<p><a href=https://jvns.ca/blog/2016/10/10/what-even-is-a-container/>What even is a container: namespaces and cgroups</a></p>
<p>video: <a href="https://www.youtube.com/watch?v=sK5i-N34im8">Cgroups, namespaces, and beyond: what are containers made from?</a></p>
</div>
</div>
</div>
</div>
<div class=footer>
<div class=container>
<div class=row>
<div class=col-12>
<div class=footer-inner>
<ul class=social>
<li><a href=https://github.com/llaoj target=blank><img width=20 height=20 src=/social/github.svg title=Github alt=Github></a></li>
</ul>
<ul class=footer-menu>
<li><a href=https://www.rutron.net/>Home</a></li>
<li><a href=https://www.rutron.net/contact>Contact</a></li>
<li class=copyright>© 2022 如创科技</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class=sub-footer>
<div class=container>
<div class=row>
<div class=col-12>
<div class=sub-footer-inner>
<ul>
<li><strong>Phone: </strong>15666139166</li>
<li><strong>Email: </strong><a href=mailto:rutronnet@163.com>rutronnet@163.com</a></li>
</ul>
<ul>
<li class=zerostatic><a href=https://beian.miit.gov.cn/#/Integrated/index>鲁ICP备2021035257号-1</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<script type=text/javascript src=/js/scripts.min.98ee06cc35517b5800b382aecb0fc59893e95b9c11dd21842d0d57e4f68043e3.js></script>
</body>
</html>